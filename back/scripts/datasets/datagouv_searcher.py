import json
import requests
import pandas as pd
import logging

from scripts.communities.communities_selector import CommunitiesSelector
from scripts.loaders.csv_loader import CSVLoader


class DataGouvSearcher():
    '''
    This class is responsible for searching datafiles on the data.gouv.fr API and datasets catalog.
    It initializes from a CommunitiesSelector object and a datagouv_config dictionary, to load the datasets and datafiles catalogs.
    It provides one public method get_datafiles(search_config, method) to build a list of datafiles based on title and description filters and column names filters.
    '''

    def __init__(self, communities_selector, datagouv_config):
        self.logger = logging.getLogger(__name__)

        self.scope = communities_selector
        self.datagouv_ids = self.scope.get_datagouv_ids() # dataframe with siren and id_datagouv columns
        self.datagouv_ids_list = self.datagouv_ids["id_datagouv"].to_list()

        # Load datagouv datasets and datafiles catalogs
        dataset_catalog_loader = CSVLoader(datagouv_config["datasets"]["url"], columns_to_keep=datagouv_config["datasets"]["columns"])
        self.dataset_catalog_df = dataset_catalog_loader.load()
        self.dataset_catalog_df = self._filter_by(self.dataset_catalog_df, "organization_id", self.datagouv_ids_list)
        # join siren to dataset_catalog_df based on organization_id
        self.dataset_catalog_df = self.dataset_catalog_df.merge(self.datagouv_ids, left_on="organization_id", right_on="id_datagouv", how="left")
        self.dataset_catalog_df.drop(columns=['id_datagouv'], inplace=True)

        datafile_catalog_loader = CSVLoader(datagouv_config["datafiles"]["url"])
        self.datafile_catalog_df = datafile_catalog_loader.load()
        self.datafile_catalog_df.columns=list(map(lambda x: x.replace("dataset.organization_id","organization_id"), self.datafile_catalog_df.columns.to_list()))
        self.datafile_catalog_df = self._filter_by(self.datafile_catalog_df, "organization_id", self.datagouv_ids_list)
        # join siren to datafile_catalog_df based on organization_id
        self.datafile_catalog_df = self.datafile_catalog_df.merge(self.datagouv_ids, left_on="organization_id", right_on="id_datagouv", how="left")
        self.datafile_catalog_df.drop(columns=['id_datagouv'], inplace=True)
        
    # Internal function to filter a dataframe by a column and one or multiple values
    def _filter_by(self, df, column, value, return_mask=False):
        # value can be a list of values or a string
        if isinstance(value, str):
            mask = df[column].str.contains(value, case=False, na=False)
        else:
            mask = df[column].isin(value)
        return mask if return_mask else df[mask]
    
    # Internal function to filter a dataframe by a column and one or multiple values
    def _get_datafiles_by_title_and_desc(self,title_filter,description_filter):
        # Get the datasets that match the title and description filters
        mask_desc = self._filter_by(self.dataset_catalog_df, "description", description_filter, return_mask=True)
        self.logger.info(f"Nombre de datasets correspondant au filtre de description : {mask_desc.sum()}")

        mask_titles = self._filter_by(self.dataset_catalog_df, "title", title_filter, return_mask=True)
        self.logger.info(f"Nombre de datasets correspondant au filtre de titre : {mask_titles.sum()}")

        # Merge the two masks and get the filtered datasets catalog
        filtered_catalog_df = self.dataset_catalog_df[(mask_titles | mask_desc)]

        # Merge with catalog files to get the filtered files list 
        filtered_files = filtered_catalog_df[["siren","id","title","description","organization","frequency"]].merge(self.datafile_catalog_df[["dataset.id","format","created_at","url"]],left_on="id",right_on="dataset.id",how="left")
        filtered_files.drop(columns=['dataset.id'], inplace=True)
        return filtered_files
    
    # Internal function to get the preferred format of a list of records
    def _get_preferred_format(self,records):
        preferred_formats = ["csv", "xls", "json", "zip"] # Could be put outside

        for format in preferred_formats:
            for record in records:
                if record.get("format: ") == format:
                    return record

        for record in records:
            if record.get("format: ") is not None:
                return record

        return records[0] if records else None


    # Internal function to create a list of dictionaries, one for each file with the specified filters of one organization
    def _get_files_by_org_from_api(self,url,organization_id,title_filter,description_filter, column_filter):
        params = {"organization": organization_id}
        scoped_files = []
        while True:
            response = requests.get(url, params=params)
            try:
                response.raise_for_status()
            except:
                break
            try:
                data = response.json()
            except json.JSONDecodeError as e:
                self.logger.error(f"Error while decoding json from {url} : {e}")
                break

            # Loop through the files list to filter them by title and description
            for result in data["data"]:
                files = []
                # Check if the title contains the title_filter words
                if any(word in result["title"].lower() for word in title_filter):
                    keyword_in_title = True
                else:
                    keyword_in_title = False

                # Check if the description contains the description_filter words
                if any(word in result["description"].lower() for word in description_filter):
                    keyword_in_description = True
                else:
                    keyword_in_description = False
                montant_col = None
                # Loop through the resources list to filter them by column names
                for resource in result["resources"]:
                    if resource["description"] is None:
                        montant_col = None
                    elif any(word in resource["description"].lower() for word in column_filter):
                        montant_col = True
                    else:
                        montant_col = False

                    # Add the file info to the files list if it matches the filters
                    files.append({"organization_id":result["organization"]["id"], "organization":result["organization"]["name"],"title":result["title"],"description":result["description"],"id":result["id"],"frequency":result["frequency"],"format":resource["format"],"url":resource["url"],"created_at":resource["created_at"],'montant_col':montant_col,"keyword_in_description":keyword_in_description,"keyword_in_title":keyword_in_title})
                # If either title, description or column name matches, add the file to the scoped_files list (check if format is in preferred formats)
                if (keyword_in_description or keyword_in_title or montant_col) and len(files)>0:
                    scoped_files.append(self._get_preferred_format(files))

            # If there is a next page, update the url and params to get the next page
            if data["next_page"]:
                url=data["next_page"]
                params = {}
            else:
                break
        return scoped_files

    # Internal function to get a list of dictionaries with the files that match the filters
    def _get_datafiles_by_content(self,url,title_filter,description_filter,column_filter):
        all_files = []

        # Loop through the organizations to get a list of dictionaries with the files that match the filters
        for orga in self.datagouv_ids_list:
            cur_files = self._get_files_by_org_from_api(url,orga,title_filter,description_filter,column_filter)
            all_files = all_files + cur_files

        bottom_up_files_df = pd.DataFrame(all_files)
        # Join with siren based on organization
        bottom_up_files_df = bottom_up_files_df.merge(self.datagouv_ids, left_on="organization_id", right_on="id_datagouv", how="left")
        bottom_up_files_df.drop(columns=['id_datagouv'], inplace=True)
        bottom_up_files_df.drop(columns=['organization_id'], inplace=True)
        return bottom_up_files_df[(bottom_up_files_df.keyword_in_title|bottom_up_files_df.keyword_in_description)&bottom_up_files_df.montant_col]
    
    # Internal function to log basic info about a search result dataframe
    def _log_basic_info(self,df):
        self.logger.info(f"Nombre de datasets correspondant au filtre de titre ou de description : {df.id.nunique()}")
        self.logger.info(f"Nombre de fichiers : {df.shape[0]}")
        self.logger.info(f"Nombre de fichiers uniques : {df.url.nunique()}")
        self.logger.info(f"Nombre de fichiers par format : {df.groupby('format').size().to_dict()}")
        self.logger.info(f"Nombre de fichiers par fr√©quence : {df.groupby('frequency').size().to_dict()}")
    
    # Function to get datafiles list selected by title and description filters and column names filters
    def get_datafiles(self, search_config, method="all"):
        # Only using topdown method: look for datafiles based on title and description filters
        if not method=="bu_only":
            topdown_datafiles = self._get_datafiles_by_title_and_desc(search_config["title_filter"],search_config["description_filter"])
            self.logger.info("Topdown datafiles basic info :")
            self._log_basic_info(topdown_datafiles)

        # Only using bottomup method: look for datafiles based on column names filters
        if not method=="td_only":
            bottomup_datafiles = self._get_datafiles_by_content(search_config["api"]["url"],search_config["api"]["title"],search_config["api"]["description"],search_config["api"]["columns"])
            self.logger.info("Bottomup datafiles basic info :")
            self._log_basic_info(bottomup_datafiles)
            
        if method == "td_only":
            datafiles = topdown_datafiles
        elif method == "bu_only":
            datafiles = bottomup_datafiles
        elif method == "all":
            # Merge topdown and bottomup: bottomup has 3 additional columns that must be dropped
            datafiles = pd.concat([topdown_datafiles, bottomup_datafiles], ignore_index=False)
            datafiles.drop_duplicates(subset=["url"], inplace=True)     # Drop duplicates based on url
            self.logger.info("Total datafiles basic info :")
            self._log_basic_info(datafiles)
        else:
            raise ValueError(f"Unknown Datafiles Searcher method {method} : should be one of ['td_only', 'bu_only', 'all']")
        
        # Add 'nom' & 'type' columns to datafiles from self.scope.selected_data based on siren
        datafiles = datafiles.merge(self.scope.selected_data[['siren', 'nom', 'type']], on='siren', how='left')
        # Add new 'source' column, filled with 'datagouv' value
        datafiles['source'] = 'datagouv'

        return datafiles